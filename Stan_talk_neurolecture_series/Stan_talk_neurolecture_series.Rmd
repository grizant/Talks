---
title: "Reasoning with Uncertainty the Bayesian way"
subtitle: "with examples in Cognitive Modeling in R and Stan"
author: "AG Schissler"
date: "2/09/2018"
output: beamer_presentation
toc: true
bibliography: library.bib
header-includes:
- \usepackage{graphicx}
- \usepackage{pdfpages}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
suppressWarnings(library(rstan))
```

# Motivation

## The problem: Science in a crisis?
- Dealing with a reproducibility crisis? [e.g., @Baker2016]
- Impact of Statistical methods for research workers @Fisher1925
- @Briggs2017, "Clients ask, 'What's the probability that if I know $X$, $Y$ will be true?'. Instead of telling them that, we give them $p$-values. This is like asking for a Cadillac and being given a broken down rickshaw without wheels."
\includegraphics[width=0.5\paperwidth]{SSI_logo.png} 

## ASA statement on $p$-values from @Wasserstein2016

1. $P$-values can indicate how incompatible the data are with a specified statistical model.
2. $P$-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
3. Scientific conclusions and business or policy decisions should not be based only on whether a $p$-value passes a specific threshold.
4. Proper inference requires full reporting and transparency
5. A $p$-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. By itself, a $p$-value does not provide a good measure of evidence regarding a model or hypothesis.

## Philosophy of Science @McElreath2016

\includegraphics[page=11,width=0.8\paperwidth]{Lecture01.pdf} 

##

\includegraphics[page=17,width=0.8\paperwidth]{Lecture01.pdf} 

## Moreover, applied statistics looks a lot like this...

\begin{columns}
\begin{column}{0.5\textwidth}  
    \begin{center}
	\begin{itemize}
		\item Most developed in early 20th century, fragile, eclipsed by more recent tools
		\item Often users don’t know they are using models
		\item Symptom of naive falsicationism
	\end{itemize}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}
	\includegraphics[height=0.9\paperheight, keepaspectratio]{stats_flow_chart_v2014.pdf}
\end{column}
\end{columns}

## What other statistical methods could one use?

- **Bayesian modeling** is highly _flexible_, _philosophically coherent_, and more readily _interpretible_.
- Dennis Lindley's Comment on _Why Isn' t Everyone a Bayesian?_ [@Efron1986], " Every statistician would be a Bayesian if he took the trouble to read the literature thoroughly and was honest enough to admit that he might have been wrong."

## 

\includegraphics[page=19,width=0.8\paperwidth]{Lecture01.pdf} 

## Bayesian methods fix everything?

- Warning from Andrew Gelman (blog 27 Jan 2018):

"... that the most important steps in any study are valid and reliable measurements and, where possible, large and stable effect sizes. All the preregistration in the world won’t save you if your measurements are not serious or if you’re studying an effect that is tiny or highly variable."

- Bayesian methods often require explicit (and sometimes numerous) assumptions and subjective belief acknowledgement.

[see @Gelman2008 for more Anti-Bayesian arguments]

# Bayes 101

## From _Statistical Rethinking_ @McElreath2016

\includegraphics[page=9,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=10,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=11,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

[comment]: ## 

[comment]: \includegraphics[page=12,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## Prior knowledge

\includegraphics[page=13,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## One observation

\includegraphics[page=14,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## Sequential updating

\includegraphics[page=15,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=16,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=17,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=18,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=19,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=20,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=21,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=22,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=23,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=24,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=25,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=26,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## 

\includegraphics[page=27,width=0.8\paperwidth]{week01_lecture02_27_Oct.pdf}

## MCMC

## Stan and Hamilition Monte Carlo

\includegraphics[page=42,width=0.8\paperwidth]{week06_lecture10_29_Nov.pdf}

# Case Study I: Inferring IQ using Gaussians

## I.1. Research question and data 

We seek to estimate the IQ of a set of people. There are only three subjects and each take three IQ tests. The data are displayed below:

```{r IQ_data}
x <- matrix(c(90, 95, 100, 105, 110, 115, 150, 155, 160), 
            nrow=3, ncol=3, byrow=T)
rownames(x) <- paste0("Subject", 1:3)
colnames(x) <- paste0("Measurement", 1:3)
kable(x)
```

## I.2. Graphical model

\begin{columns}
\begin{column}{0.5\textwidth}  
    \begin{center}
    \includegraphics[width=\columnwidth, keepaspectratio]{IQ_graph_model.pdf}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}
	$\mu_{i} \sim Normal(0,300)$
	$\sigma \sim Uniform(0,100)$
	$xy_{ij} \sim Normal(\mu_{i}, \sigma)$
\end{column}
\end{columns}

## I.3. Stan model Code 

```{r IQ_stan_model, echo = TRUE}
model <- "
// Repeated Measures of IQ
data { 
  int<lower=1> N;
  int<lower=1> P;
  matrix[N, P] x;
}
parameters {
  real mu[N];
  real<lower=0> sigma;
} 
model {
  // Data Come From Gaussians With Different Means
  // But Common Standard Deviation
  mu[N] ~ normal(100,20);
  sigma ~ exponential(0.1);
  for (i in 1:N)
    for (j in 1:P)  
      x[i,j] ~ normal(mu[i], sigma);
}"
```

## I.3  Code

```{r IQ_presample, echo = TRUE, cache = T}

N <- nrow(x) # number of people
P <- ncol(x) # number of repeated measurements

data <- list(x=x, N=N, P=P) # to be passed on to Stan
myinits <- list(
  list(mu=rep(100, N), sigma=1))

# parameters to be monitored: 
parameters <- c("mu", "sigma")
```

## I.3  Code
```{r IQ_sampling, echo = TRUE, cache = T, results = "hide"}

# The following command calls Stan with specific options.
# For a detailed description type "?rstan".
samples <- stan(model_code=model,   
                data=data, 
                init=myinits,  # If not specified, gives random inits
                pars=parameters,
                iter=2000, 
                chains=1, 
                thin=1,
                # warmup = 100,  # Stands for burn-in; Default = iter/2
                # seed = 123  # Setting seed; Default is random seed
                )
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.
```

## I.4. Output and discussion
```{r IQ_output1, echo = FALSE}
kable(print(samples))
```
## I.5. Output and discussion
```{r IQ_output2, echo = FALSE, cache = TRUE}
stan_dens(samples)
```

## I.5. Under a community of priors?

# Case Study II: Hierachical signal detection

## II.1. Research question and data 

Subjects are presented with stimuli that are either signal or noise and asked to make a decision whether the trial features signal or noise. The results of the experiment for the $i^{the}$ subject can be tabulated in a simple $2 \times 2$ table:

	| |Signal|Noise|
	|---|---|---|
	|Yes response| Hit | False alarm|
	|No response| Miss | Correct rejection|
 

## II.1. Research question and data 

```{r SDT_data}
source("~/OneDrive - University of Nevada, Reno/Research/RStan/example-models/Bayesian_Cognitive_Modeling/CaseStudies/SignalDetection/heit_rotello.RData") #loads the data
names(std_i) <- c("hits", "false alarms", "misses", "correct rejections")
kable(head(std_i))
```

## II.2. Graphical model


\begin{columns}
\begin{column}{0.5\textwidth}  
    \begin{center}
    \includegraphics[width=\columnwidth, keepaspectratio]{hierarchical_signal_detection_graphical_model.jpg}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}
	$\mu_{i} \sim Normal(0,300)$
	$\sigma \sim Uniform(0,100)$
	$xy_{ij} \sim Normal(\mu_{i}, \sigma)$
\end{column}
\end{columns}


## II.3. Stan model Code

```{r SDT_stan_model, echo = TRUE}
model <- "
// Hierarchical Signal Detection Theory
data { 
  int<lower=1> k;
  int<lower=0> h[k];
  int<lower=0> f[k];
  int<lower=0> s;
  int<lower=0> n;
}
parameters {
  vector[k] d;
  vector[k] c;
  real muc;
  real mud;
  real<lower=0> lambdac;
  real<lower=0> lambdad;
} 

transformed parameters {
  real<lower=0,upper=1> thetah[k];
  real<lower=0,upper=1> thetaf[k];
  real<lower=0> sigmac;
  real<lower=0> sigmad;
  
  sigmac = inv_sqrt(lambdac);
  sigmad = inv_sqrt(lambdad);
  
  // Reparameterization Using Equal-Variance Gaussian SDT
  for(i in 1:k) {
    thetah[i] = Phi(d[i] / 2 - c[i]);
    thetaf[i] = Phi(-d[i] / 2 - c[i]);
  }
}
model {
  // Priors 
  muc ~ normal(0, inv_sqrt(.001));
  mud ~ normal(0, inv_sqrt(.001));
  lambdac ~ gamma(.001, .001);
  lambdad ~ gamma(.001, .001);
  
  // Discriminability and Bias
  c ~ normal(muc, sigmac);
  d ~ normal(mud, sigmad);
  // Observed counts
  h ~ binomial(s, thetah);
  f ~ binomial(n, thetaf);
}"
```

## II.3  Code

```{r SDT_presample, echo = TRUE, cache = T}
niter   <- 10000
nburnin <- 1000
```

## II.3  Code
```{r SDT_sampling, echo = TRUE, cache = T, results = "hide"}

for (dataset in 1:2) {  #analyze both conditions

  if (dataset == 1)
    data <- std_i # the induction data
  if (dataset == 2)
    data <- std_d # the deduction data
  
  h <- data[, 1]
  f <- data[, 2]
  MI <- data[, 3]
  CR <- data[, 4]
  s <- h + MI
  n <- f + CR
  s <- s[1]; n <- n[1] #Each subject gets same number of signal and noise trials 
  k <- nrow(data) 

  data <- list(h=h, f=f, s=s, n=n, k=k) # To be passed on to Stan

  myinits <- list(
    list(d=rep(0, k), c=rep(0, k), mud=0, muc=0, lambdad=1, lambdac=1)) 

  # Parameters to be monitored
  parameters <- c("mud", "muc", "sigmad", "sigmac")
  
  if (dataset == 1) {
    # The following command calls Stan with specific options.
    # For a detailed description type "?rstan".
    isamples <- stan(model_code=model,   
                     data=data, 
                     init=myinits,  # If not specified, gives random inits
                     pars=parameters,
                     iter=niter, 
                     chains=1, 
                     thin=1,
                     warmup=nburnin,  # Stands for burn-in; Default = iter/2
                     # seed=123  # Setting seed; Default is random seed
    )
  }
  if (dataset == 2) {
    # The following command calls Stan with specific options.
    # For a detailed description type "?rstan".
    dsamples <- stan(fit=isamples,   
                     data=data, 
                     init=myinits,  # If not specified, gives random inits
                     pars=parameters,
                     iter=niter, 
                     chains=1, 
                     thin=1,
                     warmup=nburnin,  # Stands for burn-in; Default = iter/2
                     # seed=123  # Setting seed; Default is random seed
    )
  }
}
# Now the values for the monitored parameters are in the "isamples" and 
# "dsamples "objects, ready for inspection.

```

## II.4. Output and discussion
```{r SDT_output1, echo = FALSE, cache = TRUE}
keepi <- 1000
keep <- sample(niter, keepi)

imud <- extract(isamples)$mud
imuc <- extract(isamples)$muc
d.imuc <- density(imuc)

dmud <- extract(dsamples)$mud
dmuc <- extract(dsamples)$muc
d.dmuc <- density(dmuc)
```

## II.5. Output and discussion
```{r SDT_output2, echo = FALSE}
#####Figure 11.5 & 11.6

layout(matrix(c(1,2,3,0),2,2,byrow=T), width=c(2/3, 1/3), heights=c(2/3,1/3))
#layout.show()

par(mar=c(2,2,1,0))
plot(imud[keep],imuc[keep], xlab="", ylab="", axes=F,xlim=c(-1,6), ylim=c(-3,3))
points(dmud[keep],dmuc[keep], col="grey")
box(lty=1)

par(mar=c(2,1,1,4))
plot(d.imuc$y, d.imuc$x, xlim=rev(c(0,2.5)),type='l', axes=F, xlab="", ylab="",
     ylim=c(-3,3))
lines(d.dmuc$y, d.dmuc$x, col="grey")
axis(4)
mtext(expression(paste(mu, "c")), side=4,line=2.3, cex=1.3)
box(lty=1)

par(mar=c(6,2,0,0))
plot(density(imud),zero.line=F ,main="", ylab="", xlab="", cex.lab=1.3, axes=F,
     xlim=c(-1,6),ylim=c(0,1))
lines(density(dmud), col="grey")
axis(1, at=c(-1, 0, 1, 2, 3, 4, 5, 6))
mtext(expression(paste(mu, "d")), side=1.2,line=2, cex=1.3)
box(lty=1)

```

## I.5. Under a community of priors?

# Case Study III: Psychophysical


## III.1. Research question and data 

Subjects are presented with stimuli that are either signal or noise and asked to make a decision whether the trial features signal or noise. The results of the experiment for the $i^{the}$ subject can be tabulated in a simple $2 \times 2$ table:

	| |Signal|Noise|
	|---|---|---|
	|Yes response| Hit | False alarm|
	|No response| Miss | Correct rejection|
 

## III.1. Research question and data 

```{r psych_data}
source("~/OneDrive - University of Nevada, Reno/Research/RStan/example-models/Bayesian_Cognitive_Modeling/CaseStudies/SignalDetection/heit_rotello.RData") #loads the data
names(std_i) <- c("hits", "false alarms", "misses", "correct rejections")
kable(head(std_i))
```

## II.2. Graphical model

\begin{columns}
\begin{column}{0.5\textwidth}  
    \begin{center}
    \includegraphics[width=\columnwidth, keepaspectratio]{psychophysical_graphical_model.jpg}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}
	$\mu_{i} \sim Normal(0,300)$
	$\sigma \sim Uniform(0,100)$
	$xy_{ij} \sim Normal(\mu_{i}, \sigma)$
\end{column}
\end{columns}


## II.3. Stan model Code

```{r psych_stan_model, echo = TRUE}

```

## III.3  Code

```{r psych_presample, echo = TRUE, cache = T}


```

## III.3  Code
```{r psych_sampling, echo = TRUE, cache = T, results = "hide"}

```

## III.4. Output and discussion
```{r psych_output1, echo = FALSE, cache = TRUE}

```

## III.5. Output and discussion
```{r psych_output2, echo = FALSE}

```

## III.6. Extension to contamination

\begin{columns}
\begin{column}{0.5\textwidth}  
    \begin{center}
    \includegraphics[width=\columnwidth, keepaspectratio]{psychophysical_containment_graphical_model.jpg}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}
	$\mu_{i} \sim Normal(0,300)$
	$\sigma \sim Uniform(0,100)$
	$xy_{ij} \sim Normal(\mu_{i}, \sigma)$
\end{column}
\end{columns}

# Conclusion and references

## Take home points
- Bayesian modeling is natural and flexible
- Tools exist to aeou
- collaboration opportunities

## Software used

## References {.allowframebreaks}
