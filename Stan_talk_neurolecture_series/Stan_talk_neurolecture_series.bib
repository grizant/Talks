@book{McElreath2016,
abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds readers' knowledge of and confidence in statistical modeling. Reflecting the need for even minor programming in today's model-based statistics, the book pushes readers to perform step-by-step calculations that are usually automated. This unique computational approach ensures that readers understand enough of the details to make reasonable choices and interpretations in their own modeling work. The text presents generalized linear multilevel models from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. It covers from the basics of regression to multilevel models. The author also discusses measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. By using complete R code examples throughout, this book provides a practical foundation for performing statistical inference. Designed for both PhD students and seasoned professionals in the natural and social sciences, it prepares them for more advanced or specialized statistical modeling. Web ResourceThe book is accompanied by an R package (rethinking) that is available on the author's website and GitHub. The two core functions (map and map2stan) of this package allow a variety of statistical models to be constructed from standard model formulas.},
author = {McElreath, Richard},
booktitle = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan},
doi = {10.3102/1076998616659752},
isbn = {1482253461},
issn = {1076-9986},
keywords = {Projects:Stan{\_}talk},
mendeley-tags = {Projects:Stan{\_}talk},
pages = {469},
title = {{Statistical Rethinking}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=mDo0CwAAQBAJ{\&}pgis=1},
year = {2016}
}
@book{Lee2013,
abstract = {First two sections of the book},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lee, Michael D. and Wagenmakers, Eric Jan},
booktitle = {Bayesian Cognitive Modeling: A Practical Course},
doi = {10.1017/CBO9781139087759},
eprint = {arXiv:1011.1669v3},
isbn = {9781139087759},
issn = {1098-6596},
keywords = {Projects:Stan{\_}talk},
mendeley-tags = {Projects:Stan{\_}talk},
pages = {1--264},
pmid = {25246403},
title = {{Bayesian cognitive modeling: A practical course}},
year = {2013}
}
@article{Zhang2017,
abstract = {For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov chain Monte Carlo methods, namely, Hamiltonian Monte Carlo. The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effec-tive approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an effi-cient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differ-ently, our method can be related to other approaches for the construction of surrogate functions such as generalized addi-tive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the-art methods.},
author = {Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai and Zhang, B Cheng},
doi = {10.1007/s11222-016-9699-1},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Zhang et al/Zhang et al.{\_}2017{\_}Hamiltonian Monte Carlo acceleration using surrogate functions with random bases.pdf:pdf},
journal = {Statistics and Computing},
keywords = {Hamiltonian dynamics,Markov chain Monte Carlo,Projects:GPU{\_}NORTA,Projects:Stan{\_}talk,Random bases,Surrogate method},
mendeley-tags = {Projects:GPU{\_}NORTA,Projects:Stan{\_}talk},
pages = {1473--1490},
title = {{Hamiltonian Monte Carlo acceleration using surrogate functions with random bases}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs11222-016-9699-1.pdf},
volume = {27},
year = {2017}
}
@article{Gelman2015,
abstract = {Stan is a free and open-source C++ program that performs Bayesian inference or optimiza- tion for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia, and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users' and developers' perspectives and illustrate with a simple but nontrivial nonlinear regression example.},
author = {Gelman, A. and Lee, D. and Guo, J.},
doi = {10.3102/1076998615606113},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Gelman, Lee, Guo/Gelman, Lee, Guo{\_}2015{\_}Stan A Probabilistic Programming Language for Bayesian Inference and Optimization.pdf:pdf},
isbn = {1076998615},
issn = {1076-9986},
journal = {Journal of Educational and Behavioral Statistics},
keywords = {Projects:Stan{\_}talk},
mendeley-tags = {Projects:Stan{\_}talk},
number = {5},
pages = {530--543},
title = {{Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization}},
url = {http://jeb.sagepub.com/cgi/doi/10.3102/1076998615606113},
volume = {40},
year = {2015}
}
@article{Gelman2014,
abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
archivePrefix = {arXiv},
arxivId = {1307.5928},
author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
doi = {10.1007/s11222-013-9416-2},
eprint = {1307.5928},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Gelman, Hwang, Vehtari/Gelman, Hwang, Vehtari{\_}2014{\_}Understanding predictive information criteria for Bayesian models.pdf:pdf},
isbn = {0960-3174},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Projects:Stan{\_}talk},
mendeley-tags = {Projects:Stan{\_}talk},
title = {{Understanding predictive information criteria for Bayesian models}},
year = {2014}
}
@article{Carpenter2017,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matt and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus A. and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Carpenter et al/Carpenter et al.{\_}2017{\_}Stan A Probabilistic Programming Language.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {Projects:Stan{\_}talk},
mendeley-tags = {Projects:Stan{\_}talk},
number = {1},
title = {{Stan: A Probabilistic Programming Language}},
volume = {76},
year = {2017}
}
